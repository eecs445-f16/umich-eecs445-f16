{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recall: Boosting\n",
    "\n",
    "### AdaBoost Algorithm\n",
    "\n",
    "An *iterative* algorithm for \"ensembling\" base learners\n",
    "\n",
    "- Input: $\\{(\\mathbf{x}_i, y_i)\\}_{i = 1}^n, T, \\mathscr{F}$, base learner\n",
    "- Initialize: $\\mathbf{w}^{1} = (\\frac{1}{n}, ..., \\frac{1}{n})$\n",
    "- For $t = 1, ..., T$\n",
    " - $\\mathbf{w}^{t} \\rightarrow \\boxed{\\text{base learner finds} \\quad \\arg\\min_{f \\in \\mathscr{F}} \\sum \\limits_{i = 1}^n w^t_i \\mathbb{1}_{\\{f(\\mathbf{x}_i) \\neq y_i\\}}} \\rightarrow f_t$\n",
    " - $\\alpha_t = \\frac{1}{2}\\text{ln}\\left(\\frac{1 - r_t}{r_t}\\right)$\n",
    "     - where $r_t := e_{\\mathbf{w}^t}(f_t) = \\frac 1 n \\sum \\limits_{i = 1}^n w_i \\mathbf{1}_{\\{f(\\mathbf{x}_i) \\neq y_i\\}} $\n",
    " - $w_i^{t + 1} = \\frac{w_i^t \\exp \\left(- \\alpha_ty_if_t(\\mathbf{x}_i)\\right)}{z_t}$ where $z_t$ normalizes.\n",
    "- Output: $h_T(\\mathbf{x}) = \\text{sign}\\left(\\sum \\limits_{t = 1}^T \\alpha_t f_t(\\mathbf{x})\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adaboost through Coordinate Descent\n",
    "\n",
    "It is often said that we can view Adaboost as \"Coordinate Descent\" on the exponential loss function.\n",
    "\n",
    "**Question**: Can you figure out what that means? Why is Adaboost doing coordinate descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Hint 1*: You need to figure out the objective function being minimized. For simplicity, assume there are a finite number of weak learners in $\\mathscr{F}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Hint 2*: Recall that the exponential loss function is $\\ell(h; (x,y)) = \\exp(-y h(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Hint 3*: Let's write down the objective function being minimized. For simplicity, assume there are a finite number of weak learners in $\\mathscr{F}$, say indexed by $j=1, \\ldots, m$. Given a weight vector $\\vec{\\alpha}$,  exponential loss over the data for this $\\vec{\\alpha}$ is: \n",
    "$$\\text{Loss}(\\vec{\\alpha}) = \\sum_{i=1}^n \\exp \\left( - y_i \\left(\\sum_{j=1}^m \\alpha_j h_j(\\vec{x}_i)\\right)\\right)$$\n",
    "Coordinate descent chooses the smallest coordiante of $\\nabla L(\\vec{\\alpha})$ and updates *only this coordinate*. Which coordinate is chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging classifiers\n",
    "\n",
    "Let's explore how bagging (bootstrapped aggregation) works with classifiers to reduce variance, first by evaluating off the shelf tools and then by implementing our own basic bagging classifier.\n",
    "\n",
    "In both examples we'll be working with the dataset from the [forest cover type prediction Kaggle competition](https://www.kaggle.com/c/forest-cover-type-prediction), where the aim is to build a multi-class classifier to predict the forest cover type of a 30x30 meter plot of land based on cartographic features. See [their notes about the dataset](https://www.kaggle.com/c/forest-cover-type-prediction/data) for more background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring bagging\n",
    "\n",
    "### Loading and splitting the dataset\n",
    "\n",
    "First, let's load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type32</th>\n",
       "      <th>Soil_Type33</th>\n",
       "      <th>Soil_Type34</th>\n",
       "      <th>Soil_Type35</th>\n",
       "      <th>Soil_Type36</th>\n",
       "      <th>Soil_Type37</th>\n",
       "      <th>Soil_Type38</th>\n",
       "      <th>Soil_Type39</th>\n",
       "      <th>Soil_Type40</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0   1       2596      51      3                               258   \n",
       "1   2       2590      56      2                               212   \n",
       "2   3       2804     139      9                               268   \n",
       "3   4       2785     155     18                               242   \n",
       "4   5       2595      45      2                               153   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                               0                              510   \n",
       "1                              -6                              390   \n",
       "2                              65                             3180   \n",
       "3                             118                             3090   \n",
       "4                              -1                              391   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm     ...      Soil_Type32  \\\n",
       "0            221             232            148     ...                0   \n",
       "1            220             235            151     ...                0   \n",
       "2            234             238            135     ...                0   \n",
       "3            238             238            122     ...                0   \n",
       "4            220             234            150     ...                0   \n",
       "\n",
       "   Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  \\\n",
       "0            0            0            0            0            0   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "4            0            0            0            0            0   \n",
       "\n",
       "   Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type  \n",
       "0            0            0            0           5  \n",
       "1            0            0            0           5  \n",
       "2            0            0            0           2  \n",
       "3            0            0            0           2  \n",
       "4            0            0            0           5  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('forest-cover-type.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we extract the X/y features and split them into a 60/40 train / test split so that we can see how well the training set performance generalizes to a heldout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = df.iloc[:, 1:-1].values, df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.6, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating train/test with and without bagging\n",
    "\n",
    "Now let's use an off the shelf decision tree classifier and compare its train/test performance with a [bagged](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree training|test accuracy: 1.00 | 0.73\n",
      "bagged tree training|test accuracy: 0.99 | 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = [\n",
    "    ('tree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('bagged tree', BaggingClassifier(\n",
    "            DecisionTreeClassifier(random_state=0),\n",
    "            random_state=0,\n",
    "            n_estimators=10))\n",
    "]\n",
    "\n",
    "for label, model in models:  \n",
    "    model.fit(X_train, y_train) \n",
    "    print(\"{} training|test accuracy: {:.2f} | {:.2f}\".format(\n",
    "            label, \n",
    "            accuracy_score(y_train, model.predict(X_train)),\n",
    "            accuracy_score(y_test, model.predict(X_test))))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that both models were able to (nearly) fit the training set perfectly, and that bagging substantially improves test set performance (reduces variance).\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Let's look at two hyperparametes associated with the bagging classifier: \n",
    "\n",
    "- **num_estimators** controls how many classifiers make up the ensemble\n",
    "- **max_samples** controls how many samples each classifier in the ensemble draws\n",
    "\n",
    "#### How many classifiers do we need to reduce variance?\n",
    "\n",
    "The default number of estimators is 10; explore the performance of the bagging classifier with a range values. How many classifiers do we need to reduce variance? What is the point of diminishing returns for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### How much of the dataset does each classifier need?\n",
    "\n",
    "By default, max_samples is set to 1.0, which means each classifier gets a number of samples equal to the size of the training set. \n",
    "\n",
    "How do you suppose bagging manages to reduce variance while still using the same number of samples?\n",
    "\n",
    "Explore how the performance varies as you range `max_samples` (note, you can use float values between 0.0 and 1.0 to choose a percentage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your code goes here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing Bagging\n",
    "\n",
    "We've shown the power of bagging, now let's appreciate its simplicity by implementing our own bagging classifier right here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "\n",
    "class McBaggingClassifier(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, classifier_factory=DecisionTreeClassifier, num_classifiers=10):\n",
    "        self.classifier_factory = classifier_factory\n",
    "        self.num_classifiers = num_classifiers\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # create num_classifier classifiers calling classifier_factory, each \n",
    "        # fitted with a different sample from X\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # get the prediction for each classifier, take a majority vote        \n",
    "        return np.ones(X.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You should be able to achieve similar performance to scikit-learn's implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree training|test accuracy: 1.00 | 0.73\n",
      "our bagged tree training|test accuracy: 0.14 | 0.14\n"
     ]
    }
   ],
   "source": [
    "our_models = [\n",
    "    ('tree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('our bagged tree', McBaggingClassifier(\n",
    "            classifier_factory=lambda: DecisionTreeClassifier(random_state=0)\n",
    "            ))\n",
    "]\n",
    "\n",
    "for label, model in our_models:  \n",
    "    model.fit(X_train, y_train) \n",
    "    print(\"{} training|test accuracy: {:.2f} | {:.2f}\".format(\n",
    "        label, \n",
    "        accuracy_score(y_train, model.predict(X_train)),\n",
    "        accuracy_score(y_test, model.predict(X_test))))    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
