{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 445: Machine Learning\n",
    "\n",
    "## Hands On 10: Bias Variance Tradeoff\n",
    "\n",
    "Consider a sequence of IID random variable: \n",
    "$$\n",
    "X_i =\n",
    "\\begin{cases}\n",
    "100 & \\text{ with prob. } 0.02 \\\\\n",
    "0 & \\text{ with prob. } 0.97 \\\\\n",
    "-100 & \\text{ with prob. } 0.01 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "The true mean of $X_i$ is \n",
    "$$\n",
    "0.02 \\times 100 + 0.97 \\times 0 + 0.01 \\times -100 = 1\n",
    "$$\n",
    "\n",
    "We want to estimate the true mean of this distribution. We will consider two different estimators of the true mean.\n",
    "Let's say you take three samples $X_1, X_2, X_3$, and you compute the **empirical mean** $Z=\\frac{X_1 + X_2 + X_3}{3}$ and **empirical median** $Y$ of these three samples (recall that the median is obtained by sorting $X_1, X_2, X_3$ and then choosing the middle (2nd) entry).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the bias-variance tradeoff of the  $Y$ and $Z$ for estimating the true mean of the above distribution?\n",
    "\n",
    "* They are both unbiased estimators of the true mean, and have the same variance.\n",
    "* The median has higher bias and higher variance.\n",
    "* The mean has higher bias and higher variance.\n",
    "* They both have no bias, but the mean has lower variance.\n",
    "* The mean has no bias but some variance, and the median has non-zero bias but less variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "> The last answer is correct.\n",
    "\n",
    "> The empirical mean of a sample of random $n$ IID random variables is always an unbiased estimate of the true mean. However, the empirical mean estimator can have high variance. Here it is $ \\text{Var}(Z) = \\frac{\\text{Var}(X_i)}{3} =  \\frac{(100-1)^2 \\times 0.02 + (-100 - 1)^2 \\times 0.01 + (0-1)^2 \\times 0.97}{3} = 99 \\frac 2 3.$\n",
    "\n",
    ">The median, on the other hand, is a biased estimator. It is a little bit hard to calculate exactly, but here goes:\n",
    "$$\n",
    "median = \\begin{cases} 100 &  w.p. 0.02^3 + \\binom{3}{1} 0.02^2 \\times 0.98 \\\\\n",
    "-100 & w.p. 0.01^3 + \\binom{3}{1} 0.01^2 \\times 0.99\n",
    "\\end{cases}\n",
    "$$\n",
    "If you work this out, you see that the median on average is $0.089$. This means that the $\\text{bias}^2 \\approx (1-0.089)^2$ which is no more than 1. Using a similar argument, you can check that the variance of the median is no more than 20. This can be checked experimentally! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Bias-Variance Tradeoff eqaution \n",
    "Assume that we have noisy data, modeled by $f = y + \\epsilon$, where $\\epsilon \\in \\mathcal{N}(0,\\sigma)$. Given an estimator $\\hat{f}$, the squared error can be derived as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left[\\left(\\hat{f} - f\\right)^2\\right] &= \\mathbb{E}\\left[\\hat{f}^2 - 2f\\hat{f} + f^2\\right]\\\\\n",
    "&= \\mathbb{E}\\left[\\hat{f}^2\\right] + \\mathbb{E}\\left[f^2\\right] - 2\\mathbb{E}\\left[f\\hat{f}^2\\right] \\text{ By linearity of expectation} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now, by definition, $Var(x) = \\mathbb{E}\\left[x^2\\right] - \\left(\\mathbb{E}\\left[x\\right]\\right)^2$. Subsituting this definition into the eqaution above, we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}\\left[\\hat{f}^2\\right] + \\mathbb{E}\\left[f^2\\right] - 2\\mathbb{E}\\left[f\\hat{f}^2\\right] &= Var(\\hat{f}) + \\left(\\mathbb{E}[\\hat{f}]\\right)^2  + Var(f) + \\left(\\mathbb{E}[f]\\right)^2 - 2f\\mathbb{E}[\\hat{F}^2] \\\\ \n",
    "&= Var(\\hat{f}) + Var(f) + \\left(\\mathbb{E}[\\hat{f}] - f\\right)^2\\\\\n",
    "&= \\boxed{\\sigma + Var(\\hat{f}) + \\left(\\mathbb{E}[\\hat{f}] - f\\right)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first term $\\sigma$ is the irreducible error due to the noise in the data (from the distribution of $\\epsilon$). The second term is the **variance** of the estimator $\\hat{f}$ and the final term is the **bias** of the estimator. There is an inherent tradeoff between the bias and variance of an estimator. Generally, more complex estimators (think of high-degree polynomials as an example) will have a low bias since they will fit the sampled data really well. However, this accuracy will not be maintained if we continued to resample the data, which implies that the variance of this estimator is high.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Bias Variance Tradeoff\n",
    "We will now see try to see the inherent tradeoff between bias and variance of estimators through linear regression. Consider the following dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.matlib import repmat\n",
    "\n",
    "from sklearn\n",
    "degrees = [1,2,3,4,5]\n",
    "\n",
    "\n",
    "#define data\n",
    "n = 20\n",
    "sub = 1000\n",
    "mean = 0\n",
    "std = 0.25\n",
    "\n",
    "#define test set\n",
    "Xtest = np.random.random((n,1))*2*np.pi\n",
    "ytest = np.sin(Xtest) + np.random.normal(mean,std,(n,1))\n",
    "\n",
    "#pre-allocate variables\n",
    "preds = np.zeros((n,sub))\n",
    "bias = np.zeros(len(degrees))\n",
    "variance = np.zeros(len(degrees))\n",
    "mse = np.zeros(len(degrees))\n",
    "values = np.expand_dims(np.linspace(0,2*np.pi,100),1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try several polynomial fits to the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j,degree in enumerate(degrees):\n",
    "    \n",
    "    for i in range(sub):\n",
    "            \n",
    "        #create data - sample from sine wave     \n",
    "        x = np.random.random((n,1))*2*np.pi\n",
    "        y = np.sin(x) + np.random.normal(mean,std,(n,1))\n",
    "        \n",
    "        #TODO\n",
    "        #create features corresponding to degree - ex: 1, x, x^2, x^3...\n",
    "        A = \n",
    "        \n",
    "        #TODO:        \n",
    "        #fit model using least squares solution (linear regression)\n",
    "        #later include ridge regression/normalization\n",
    "        coeffs = \n",
    "                \n",
    "        #store predictions for each sampling\n",
    "        preds[:,i] = poly.fit_transform(Xtest).dot(coeffs)[:,0]\n",
    "        \n",
    "        #plot 9 images\n",
    "        if i < 9:\n",
    "            plt.subplot(3,3,i+1)\n",
    "            plt.plot(values,poly.fit_transform(values).dot(coeffs),x,y,'.b')\n",
    "\n",
    "    plt.axis([0,2*np.pi,-2,2])\n",
    "    plt.suptitle('PolyFit = %i' % (degree))\n",
    "    plt.show()\n",
    "\n",
    "    #TODO\n",
    "    #Calculate mean bias, variance, and MSE (UNCOMMENT CODE BELOW!)\n",
    "    #bias[j] = \n",
    "    #variance[j] = \n",
    "    #mse[j] = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data with the estimators! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(3,1,1)\n",
    "plt.plot(degrees,bias)\n",
    "plt.title('bias')\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(degrees,variance)\n",
    "plt.title('variance')\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(degrees,mse)\n",
    "plt.title('MSE')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
